{"cells":[{"cell_type":"code","source":["!pip install umap-learn\n","!pip install pytorch-lightning"],"metadata":{"id":"R_JyNFEm1_Ur"},"id":"R_JyNFEm1_Ur","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"31f90f36-4fc2-4a5b-a403-5049d778a21e","metadata":{"id":"31f90f36-4fc2-4a5b-a403-5049d778a21e"},"outputs":[],"source":["import tarfile\n","from PIL import Image\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import datasets, transforms\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import umap\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"id":"661e7330-981a-4480-b4d9-70f1cde45d9b","metadata":{"id":"661e7330-981a-4480-b4d9-70f1cde45d9b"},"outputs":[],"source":["import matplotlib\n","font = {'weight' : 'regular',\n","        'size'   : 22}\n","\n","matplotlib.rc('font', **font)"]},{"cell_type":"code","execution_count":null,"id":"80a4610b-a925-4520-a5cb-84bc8c8b53c1","metadata":{"id":"80a4610b-a925-4520-a5cb-84bc8c8b53c1","outputId":"49c9a861-b2e9-4587-f164-571c7a971707","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647956354834,"user_tz":240,"elapsed":650,"user":{"displayName":"Rubert Guillermo Martín Pardo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfYAEjukvowzmWimRXfinZ_602oEpPfXwQu1sNJA=s64","userId":"17460270256521479479"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  1635  100  1635    0     0   4605      0 --:--:-- --:--:-- --:--:--  4605\n"]}],"source":["!curl -O https://dl.sphericalcow.xyz/ecse552/T4/dict_logger.py\n","from dict_logger import DictLogger"]},{"cell_type":"markdown","id":"b5ec8f86-9975-418a-9e54-65db797dc8d0","metadata":{"id":"b5ec8f86-9975-418a-9e54-65db797dc8d0"},"source":["# ECSE552 - W22 - Homework 3\n","Released: 16 March 2022\n","\n","Due: 30 March 2022\n","\n","Last Modified: 15 March 2022"]},{"cell_type":"markdown","id":"e77b5d54-1b01-49e0-9a44-c59cccb23de0","metadata":{"id":"e77b5d54-1b01-49e0-9a44-c59cccb23de0"},"source":["# Part 1 - Conditional Variational Autoencoders\n","\n","### Introduction\n","\n","Conditional Variational Autoencoders (CVAEs) are an extension of Variational Autoencoders (VAEs).\n","\n","As we explored in tutorial eight, VAEs are comprised of two main parts: an encoder and a decoder.\n","\n","The encoder approximates the function $Q(z|X)$ and the decoder approximates the function $P(X|z)$.\n","\n","As a reminder, here $X$ is the input data and $z$ is a latent vector.\n","\n","For more details, you might consider this [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) by Carl Doersch.\n","\n","The above formulation, however, does not allow us to generate specific images. For example, given a model trained on the MNIST digits, it's difficult to fully explore the latent space for a given digit.\n","\n","We can solve this by conditioning $P$ by not only the latent vector $z$, but some value $c$ that can encode information, (_e.g._: which MNIST digit we're generating).\n","\n","This would mean our decoder would got from $P(X|z)$ in the VAE case to $P(X|z,c)$ in the CVAE case.\n","\n","### Practical Implementation\n","\n","Practically, this means that a \"conditional\" vector $c$ is passed to the decoder. This is commonly a class label for the sample that is one-hot encoded, but it can be anything you wish to marginalise the model by.\n","\n","Below is a very high-level illustration of both a Variational Autoencoder (VAE) and a Conditional Variational Autoencoder (CVAE). Components of the CVAE that are not present in the VAE are demarked with a yellow star.\n","\n","The CVAE is identical to the VAE, but for the concatenation of the vector $c$ to the vector $z$ before being inputted into the decoder. The loss function does not change in any way.\n","\n","In this diagram the dimensions are noted for all the inputs and outputs of the Encoder and Decoder.\n","\n","The dimension variables are as follows:\n","\n","* $m$ is the batch size\n","* $w$ is the width of the image inputted into our encoder\n","* $h$ is the height of the image inputted into our encoder\n","* $n$ is the number of latent variables\n","* $q$ is the width of the conditional vector. If $c$ represents the class of the image, then $q$ is often equal to the number of classes.\n","\n","![VAE vs CVAE](https://dl.sphericalcow.xyz/ecse552/W22/H3/vae_cvae.png)\n","\n","Let's look closer at the concatenation of $z$ and $c$, as well as the representation of $c$. Consider the case of a CVAE where the samples are MNIST digits and $c$ is the one-hot encoded class of the digit. In this example, the number of latent variables ($n$ in the diagram above) is 3. Below is an illustration of what the concatenation might look like.\n","\n","![Concatenating z with c](https://dl.sphericalcow.xyz/ecse552/W22/H3/conditoinal_z.png)\n","\n","The vectors that are the result of concatenating $z$ and $c$ (let's call it $z_c$) then become the input to the decoder."]},{"cell_type":"markdown","id":"129b729e-c5c3-4eda-8fb8-09dc9882505c","metadata":{"id":"129b729e-c5c3-4eda-8fb8-09dc9882505c"},"source":["### Q1.1 - Implementing an MNIST CVAE (26 pts)\n","\n","Your task is to extend the MNIST VAE from Tutorial 8 to a CVAE.\n","\n","Specifically, you must:\n","\n","1. Correctly define a conditional vector $c$ in the training loop.\n","2. Correctly concatenate it to $z$ to form $z_c$.\n","3. Input $z_c$ to the Decoder.\n","4. Make any further nescessary modifications to the code.\n","\n","You are provided with the complete code for the MNIST VAE presented in Tutorial 8, including data loaders, encoders, decoders, and training/validation loops.\n","\n","The number of latent variables for this excercise ($n$) is set to 10.\n","\n","**Hint**: While you are permitted to make changes to the Encoder and Decoder class, it is not nescessary to correctly answer this question.\n","\n","**Another Hint**: You can correctly answer this question by modifying/adding no more than six lines."]},{"cell_type":"code","execution_count":null,"id":"3b7dff07-e634-4c4a-8abe-5438bd2de14d","metadata":{"id":"3b7dff07-e634-4c4a-8abe-5438bd2de14d"},"outputs":[],"source":["class MNISTDataset(Dataset):\n","\n","    def __init__(self, csv_path):\n","\n","        self.df = pd.read_csv(csv_path)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \n","        return torch.tensor(self.df.iloc[idx][1:])/255, self.df.iloc[idx][0]"]},{"cell_type":"code","execution_count":null,"id":"48e82a9e-2373-41ef-866d-9c22f1e0c338","metadata":{"id":"48e82a9e-2373-41ef-866d-9c22f1e0c338"},"outputs":[],"source":["batch_size = 100\n","\n","dataset = MNISTDataset('./sample_data/mnist_train_small.csv')\n","\n","num_test = len(dataset) // 10\n","num_train = len(dataset) - num_test\n","dataset_train, dataset_test = random_split(dataset, [num_train, num_test])\n","\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"6e022cf9-7029-4fa3-8d0b-85132cedcbbe","metadata":{"id":"6e022cf9-7029-4fa3-8d0b-85132cedcbbe"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, in_dim, latent_dim):\n","        super(Encoder, self).__init__()\n","\n","        self.layer0 = nn.Linear(in_dim, 1024)\n","        self.layer1 = nn.Linear(512, 768)\n","        self.layer2 = nn.Linear(768, 1024)\n","        self.layer3 = nn.Linear(512, 256)\n","        self.layer4 = nn.Linear(128, latent_dim)\n","\n","        self.pool = nn.MaxPool1d(2)\n","\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.activation = nn.LeakyReLU(0.01)\n","\n","    def forward(self, x):\n","\n","        x = self.layer0(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.shape[0], 1, x.shape[1])\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], x.shape[2])\n","\n","        x = self.layer1(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer2(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.shape[0], 1, x.shape[1])\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], x.shape[2])\n","\n","        x = self.layer3(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.shape[0], 1, x.shape[1])\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], x.shape[2])\n","\n","        x = self.layer4(x)\n","        x = self.activation(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"a9980357-db64-4e8f-9e6e-26dff27e6155","metadata":{"id":"a9980357-db64-4e8f-9e6e-26dff27e6155"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, out_dim):\n","        super(Decoder, self).__init__()\n","\n","        self.layer4 = nn.Linear(784, out_dim)\n","        self.layer3 = nn.Linear(768, 784)\n","        self.layer2 = nn.Linear(1024, 768)\n","        self.layer1 = nn.Linear(256, 1024)\n","        self.layer0 = nn.Linear(latent_dim, 256)\n","\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.activation = nn.LeakyReLU(0.01)\n","\n","    def forward(self, x):\n","\n","        x = self.layer0(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer1(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer2(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer3(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer4(x)\n","        x = torch.sigmoid(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"126f99c3-ba20-40ec-8872-77f58a90447b","metadata":{"id":"126f99c3-ba20-40ec-8872-77f58a90447b"},"outputs":[],"source":["class MNIST_VAE(pl.LightningModule):\n","    def __init__(self, in_dim, out_dim, latent_dim):\n","        super(MNIST_VAE, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        \n","        self.encoder = Encoder(in_dim, latent_dim*2)\n","        self.decoder = Decoder(latent_dim, out_dim)\n","        self.recon_criterion = nn.BCELoss(reduction='sum')\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) \n","\n","    def loss(self, mu, logvar, pred, target):\n","\n","        kld_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n","        recons_loss = self.recon_criterion(pred, target)\n","\n","        loss = recons_loss + kld_loss\n","\n","        return kld_loss, recons_loss, loss\n","\n","    def forward(self, x):\n","\n","        parameters= self.encoder(x)\n","\n","        mu = parameters[:, :self.latent_dim]\n","        logvar = parameters[:, self.latent_dim:]\n","\n","        z = self.reparameterize(mu, logvar)\n","\n","        x_hat = self.decoder(z)\n","\n","        return mu, logvar, z, x_hat\n","\n","    def training_step(self, batch, batch_idx):\n","\n","        x, y = batch\n","        \n","        mu, logvar, z, x_hat = self(x)\n","\n","        kld_loss, recons_loss, loss = self.loss(mu, logvar, x_hat, x)\n","\n","        self.log('training_loss', loss, on_step=False, on_epoch=True)\n","        self.log('training_kld_loss', kld_loss, on_step=False, on_epoch=True)\n","        self.log('training_recons_loss', recons_loss, on_step=False, on_epoch=True)\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","\n","        x, y = batch\n","\n","        mu, logvar, z, x_hat = self(x)\n","\n","        kld_loss, recons_loss, loss = self.loss(mu, logvar, x_hat, x)\n","\n","        self.log('val_loss', loss, on_step=False, on_epoch=True)\n","        self.log('val_kld_loss', kld_loss, on_step=False, on_epoch=True)\n","        self.log('val_recons_loss', recons_loss, on_step=False, on_epoch=True)\n","        return loss\n","    \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(dataloader_train), epochs=20)\n","        return [optimizer], [scheduler]"]},{"cell_type":"code","execution_count":null,"id":"3cdb208d-c705-433e-b605-708b4f235adc","metadata":{"id":"3cdb208d-c705-433e-b605-708b4f235adc"},"outputs":[],"source":["model = MNIST_VAE(784, 784, 10)"]},{"cell_type":"code","execution_count":null,"id":"6f34edcb-4812-46ca-8483-9dd5052aa4d0","metadata":{"id":"6f34edcb-4812-46ca-8483-9dd5052aa4d0"},"outputs":[],"source":["logger = DictLogger()\n","early_stopping = EarlyStopping('val_loss', verbose=True, min_delta=0.0001, patience=3)\n","trainer = pl.Trainer(gpus=1, logger=logger, progress_bar_refresh_rate=10, \n","                     callbacks=[early_stopping], max_epochs=40)\n","\n","\n","trainer.fit(model, dataloader_train, dataloader_test)"]},{"cell_type":"code","execution_count":null,"id":"e8bdaa2c-d727-42dc-8c81-0508b7f49591","metadata":{"id":"e8bdaa2c-d727-42dc-8c81-0508b7f49591"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(logger.metrics['training_loss'], label='Training', lw=3)\n","plt.plot(logger.metrics['val_loss'], label='Val', lw=3)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"65bea93e-5567-4b61-94fa-7060f2a4f265","metadata":{"id":"65bea93e-5567-4b61-94fa-7060f2a4f265"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(logger.metrics['training_kld_loss'], label='KLD', lw=3)\n","plt.plot(logger.metrics['training_recons_loss'], label='Recon', lw=3)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"ee6f65b5-45ae-49b5-93f2-012646b8051d","metadata":{"id":"ee6f65b5-45ae-49b5-93f2-012646b8051d"},"source":["### Q1.2 Demonstrate Reconstruction (11 pts)\n","\n","In the same way as was demonstrated in Tutorial 8, the ability of your model to reconstruct the first two digits from the first batch provided by `dataloader_test`. \n","\n","Specifically, pass the first two values of `x` and `y` from `dataloader_test` to `model` and reconstruct these images. Plot them as illustrated below:\n","\n","![Reconstructed MNIST digits](https://dl.sphericalcow.xyz/ecse552/W22/H3/q1.2.png)\n","\n","**Note**: The batches are randomised so you won't nescessarily get the same samples shown above."]},{"cell_type":"code","execution_count":null,"id":"04ec42b6-2f6e-40dc-8e4c-1069d42e25ca","metadata":{"id":"04ec42b6-2f6e-40dc-8e4c-1069d42e25ca"},"outputs":[],"source":["# Q1.2 Code Here"]},{"cell_type":"markdown","id":"8632685c-46df-4553-b476-f8bcd7f4d959","metadata":{"id":"8632685c-46df-4553-b476-f8bcd7f4d959"},"source":["### Q1.3 Demonstrate Conditional Reconstruction (13 pts)\n","\n","Similar to Q1.2, reconstruct digits, but instead of using the correct $c$ vector, change the class label to a different one.\n","\n","For example, the value of `y` for the two samples below are 3 and 6, but when reconstructing, the $c$ vector `[0,0,1,0,0,0,0,0,0,0]` was supplied.\n","\n","![Conditioned Reconstruction](https://dl.sphericalcow.xyz/ecse552/W22/H3/q1.3.png)"]},{"cell_type":"code","execution_count":null,"id":"e5202413-1656-4a97-b36d-b123f212e7b2","metadata":{"id":"e5202413-1656-4a97-b36d-b123f212e7b2"},"outputs":[],"source":["# Q1.3"]},{"cell_type":"markdown","id":"2b392e21-21ed-4ae4-a6a6-6edfc0f4bafd","metadata":{"id":"2b392e21-21ed-4ae4-a6a6-6edfc0f4bafd"},"source":["### Q1.4 Explore the Latent Space (26 pts)\n","\n","We now wish to traverse the latent space by individually sweeping each latent variable within a standard deviation. By doing so with a fixed value of $c$, we can better understand what individual latent variables encode for with regard to a specific digit.\n","\n","Specifically, your task is to:\n","\n","1. Calculate the mean ($\\mu$) of latent vectors `z` from one batch provided by `dataloader_test`\n","2. Calculate the stdev ($\\sigma$) of latent vectors `z` from one batch provided by `dataloader_test`\n","3. For each combination of $i \\in [1\\dots n]$ and $j \\in [1\\dots10]$:\n","    1. Create a [clone](https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html#torch.Tensor.clone) of the $\\mu$ vector named $z^{*(i,j)}$\n","    2. Calculate $\\delta = \\frac{(\\mu+2\\sigma) - (\\mu - 2\\sigma)}{9}$\n","    3. Make the $i$th element of $z^{*(i,j)}$ equal to $\\mu_i-2\\sigma_i + \\delta_i \\times j$ \n","    4. Input into our model's decoder the latent vector $z^{*(i,j)}$ and a $c$ vector that encodes for the number 2 to create an image $\\hat{x}^{*(i,j)}$\n","4. Plot every $\\hat{x}^{*(i,j)}$ in a $10\\times10$ grid, where each row is a different value of $i$, and each column is a different value of $j$\n","\n","Recall, $n$ is the number of latent variables."]},{"cell_type":"code","execution_count":null,"id":"410a7872-6ad6-4769-9185-91a9bf41c941","metadata":{"id":"410a7872-6ad6-4769-9185-91a9bf41c941"},"outputs":[],"source":["# Q1.4 Code Here"]},{"cell_type":"markdown","id":"bd0084ec-ed4b-4402-ad3b-143abb81ba4d","metadata":{"id":"bd0084ec-ed4b-4402-ad3b-143abb81ba4d"},"source":["# Part 2: Graph Convolution Networks (GCN)\n","\n","The node update equation for a Graph Convolution layer is given by\n","\n","\\begin{align}\n","h_u^{(l+1)} = σ(b^l + ∑_{v \\in N(u)}\\frac{1}{c_{u,v}} W^{(l)}h_v^{(l)}), \n","\\end{align}\n","\n","where $u$ is the node, $l$ is the layer, $b$ is the bias, and $W$ is the trainable parameter matrix. Here, $h^{(0)}$ is the node feature.\n","\n","A 2D Convolution layer without kernel flipping is given by\n","\n","\\begin{align}\n","S^{(l+1)}(i, j) &= σ((S^{(l)}*K^{(l)})(i,j) + b^{(l)})\\\\\n"," &=  σ (  ∑_m ∑_n S^{(l)}(i+m,j+n) K^{(l)}(m,n) + b^{(l)} ),\n","\\end{align}\n","\n","where $S$ is the feature map, and $K$ is the kernel. Here $S^{(0)}=I$ (i.e. original image).\n","\n","\n","\n","### Question 2.1: Assuming that there is no 2D pooling, under which conditions are GraphConv and 2DConv equivalent? (13 pts)\n","\n","Due to the nature of the Conv2d, the equivalent graph of an image representation (or 2d feature map) $S^l$ nees to relate each node (pixel) to its spatial neighbours according to the extension of Kernel $K^l(m,n)$. It this kernel is of size MXN, this means that for a node (pixel) $v$, to be neighbour of a target pixel $u$, its coordinates $i',j'$ must be related to those of the target pixel $i,j$ according to\n","\n","\\begin{equation}\n"," i-M/2 \\le i' \\le i + M/2, \\quad  j-N/2 \\le j' \\le j + N/2\n","\\end{equation}\n","\n","Here, we will utilise interchangeably one-index notation $u$ and $v$, or double index notation $(i,j)$ and $(i',j')$, depending on which representation (graph or image) and (node or pixel) is more convenient at each point. These coordinate system can easily be transformed from one to the other according to the rule\n","\n","\\begin{equation}\n"," u = i*W + j\n","\\end{equation}\n","where W is the width of the image.\n","\n","For simplicity, we will assume a single 2D Kernel, which implies that both input and output images have single channels, and each pixel contains a single value (pixel intensity) which corresponds to single-feature nodes in the equivalent graph. \n","\n","\n","Comparing equatoins for Conv2D and GCN, it is clear that both $\\sigma$ and $b_l$ have to be the same in both representations. \n","\n","Also, since there is only one feature per node, both in the previous and in the next representation of the graph ($l-1$ and $l$), then the weight matrix neccesarily has to be a 1X1 matrix, i.e., a scalar. Therefore, it can be \"absorbed\" inside the normalization coefficients $c_{u,v}$, if we assume $W = 1$.\n","\n","We see also that the normalization coefficient $c_{u,v}$ between target node $u$ and neighbour node $v$ and the kernel indices $K(i,j)$ must satisft the relation \n","\n","\\begin{equation}\n","  ∑_{v} \\frac{h_v^l}{c_{u,v}} = ∑_{m,n} K(m,n) h_{m*L+n}\n","\\end{equation}\n","\n","where the neighbour message $h_{m*L+n}$ corresponds to the image pixel $S(i+m,j+n)$. For this expression to hold for all pixels (nodes) , we have to require that\n","\n","\\begin{equation}\n"," \\frac{1}{c_{u,v}} = constant = K(m,n) \n","\\end{equation}\n","for all elements m,n of the Kernel, and for all neighbour, target combination u,v.\n","\n","Therefore, for both Conv2D and GCN to be equivalent, we must use a Kernel on which all elements are required to be equal to a single constant, which in turn must be equal to the normalization factors $1/c_{u,v}$ (also constant for all u and v) in the GCN.\n","\n","### Question 2.2: Assuming that there is a 2D pooling after the 2D convolution, under which conditions are GraphConv and 2DConv+2D pool equivalent? (11 pts)\n","We are looking for hyperparameters, graph properties, and kernel properties. Show your reasoning. "]},{"cell_type":"code","source":[""],"metadata":{"id":"74C9Qxce38XE"},"id":"74C9Qxce38XE","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"ECSE552 - W22 - H3_Release.ipynb","provenance":[{"file_id":"1VYEfWm1Udbs7HzsDAcQOON3ivhSaascw","timestamp":1647956137250}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}